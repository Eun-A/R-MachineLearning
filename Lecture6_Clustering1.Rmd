---
title: "Lecture6_Clustering.r"
output: html_notebook
---


```{r}
####### Clustering #######
# install.packages("NbClust")
# install.packages("cluster")
# install.packages("factoextra")
# install.packages("clValid")
# install.packages("Rcpp")

suppressMessages({
library(NbClust)
library(cluster)
library(factoextra)
library(clValid)
})
```

### 1. 최적의 k 값 찾기
: K-means알고리즘에서 최적의 **k값 (군집의 수, center값)** 찾기  

<br>

####1-1. 데이터 불러오기 & 정리하기

```{r}
#data(wine, package="rattle")
head(wine)
df <- scale(wine[-1])  # 항상 scaling 해주기
```
<br>


####1-2. k값과 초기값(nstart) 설정하기

```{r}
# k-means 기본값
k.means.fit <- kmeans(df, 3) # k=3 인 경우에 k-means
k.means.fit.5 <- kmeans(df, 5) # k=5 인 경우에 k-means
k.means.fit.3.25 <- kmeans(df, 3, nstart = 25) # 다른 초기값을 25개 셋 시도

#attributes(k.means.fit)
#k.means.fit$centers
#k.means.fit$cluster
#k.means.fit$size
```
<br>
다음은 k=3 일때의 k-means알고리즘 모델 객체를 호출한 결과이다.
K-평균 군집 결과를 일목요연하게 볼 수 있다. 

- **K-means clustering with 3 clusters of sizes 61, 68, 49**

   : 군집의 개수(k)가 3개, 군집 1,2,3별 크기가(개체 개수) 61개, 68개, 49개

- **Cluster means**

   : 군집 1,2,3 별 변수들의 평균 좌표 (=> profiling 하기에 좋다.)

- **Clustering vector**

   : 각 개체별 군집 벡터
   
- **Within cluster sum of squares by cluster**

   : 각 군집의 중심(centroid)와 각 군집에 속한 개체간 거리의 제곱합

- **Available components**

   : 군집분석 결과의 구성요소
     => 필요한 구성요소가 있으면 이 객체(object)를 indexing해서 쓰면 요긴함

```{r}
k.means.fit
```
```{r}
k.means.fit.5
```
```{r}
k.means.fit.3.25
```
<br>

####1-3. Elbow method로 적절한 k 값 찾기

```{r}
wssplot <- function(data, nc=15, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="# of Clusters",
       ylab="Within group sum of squares")}

wssplot(df)
wssplot(df, nc = 6)

```
결과 그래프에서 k=3이 적정하다고 나온다.

<br>

####2-1. 2차원 평면에 k-means 결과값 도식화
```{r}
clusplot(df, k.means.fit$cluster, main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,
         labels=2, lines=0)

clusplot(df, k.means.fit.5$cluster, main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,
         labels=2, lines=0)
```
k=3으로 설정했을 때보다 그래프에서 겹치는 부분이 많다.

```{r}
# confusion matrix
table(wine[,2],k.means.fit$cluster)

table(wine[,2],k.means.fit.5$cluster)
```

<br>

####2-2. Hierarchical clustering 으로 접근

```{r}
d <- dist(df, method = "euclidean") # Euclidean계산법으로 거리행렬 구하기 
H.fit <- hclust(d, method = "ward.D2") # Ward.D2는 within-cluster variance를 최소화
                                       # hclust의 첫번째 인자는 요소간의 거리행렬이어야 함 

plot(H.fit) # dendogram 도식화
groups <- cutree(H.fit, k = 3) # cutree거리(h)나 군집 수(k) 기준으로 그룹화한 결과 데이터 얻기
rect.hclust(H.fit, k = 3, border = "red") # 군집의 수(k)가 3이 되도록 군집화된 결과 나누기 

# Confusion matrix
table(wine[,2],groups)
```


