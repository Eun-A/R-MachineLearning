---
title: "Lecture6_Clustering"
output: rmarkdown::github_document
---


```{r}
####### Clustering #######
# install.packages("NbClust")
# install.packages("cluster")
# install.packages("factoextra")
# install.packages("clValid")
# install.packages("Rcpp")

suppressMessages({
library(NbClust)
library(cluster)
library(factoextra)
library(clValid)
})
```

### 1. 최적의 k 값 찾기
: K-means알고리즘에서 최적의 **k값 (군집의 수, center값)** 찾기  

<br>

####1-1. 데이터 불러오기 & 정리하기

```{r}
#data(wine, package="rattle")
wine <- read.csv("wine.csv")
head(wine)
df <- scale(wine[-1])  # 항상 scaling 해주기
```
<br>


####1-2. k값과 초기값(nstart) 설정하기

```{r}
# k-means 기본값
k.means.fit <- kmeans(df, 3) # k=3 인 경우에 k-means
k.means.fit.5 <- kmeans(df, 5) # k=5 인 경우에 k-means
k.means.fit.3.25 <- kmeans(df, 3, nstart = 25) # 다른 초기값을 25개 셋 시도

#attributes(k.means.fit)
#k.means.fit$centers
#k.means.fit$cluster
#k.means.fit$size
```
<br>
다음은 k=3 일때의 k-means알고리즘 모델 객체를 호출한 결과이다.
K-평균 군집 결과를 일목요연하게 볼 수 있다. 

- **K-means clustering with 3 clusters of sizes 61, 68, 49**

   : 군집의 개수(k)가 3개, 군집 1,2,3별 크기가(개체 개수) 61개, 68개, 49개

- **Cluster means**

   : 군집 1,2,3 별 변수들의 평균 좌표 (=> profiling 하기에 좋다.)

- **Clustering vector**

   : 각 개체별 군집 벡터
   
- **Within cluster sum of squares by cluster**

   : 각 군집의 중심(centroid)와 각 군집에 속한 개체간 거리의 제곱합

- **Available components**

   : 군집분석 결과의 구성요소
     => 필요한 구성요소가 있으면 이 객체(object)를 indexing해서 쓰면 요긴함

```{r}
k.means.fit
```
```{r}
k.means.fit.5
```
```{r}
k.means.fit.3.25
```
<br>

####1-3. Elbow method로 적절한 k 값 찾기

```{r}
wssplot <- function(data, nc=15, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="# of Clusters",
       ylab="Within group sum of squares")}

wssplot(df)
wssplot(df, nc = 6)

```
결과 그래프에서 k=3이 적정하다고 나온다.

<br>

####2-1. 2차원 평면에 k-means 결과값 도식화
```{r}
clusplot(df, k.means.fit$cluster, main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,
         labels=2, lines=0)

clusplot(df, k.means.fit.5$cluster, main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,
         labels=2, lines=0)
```
k=3으로 설정했을 때보다 그래프에서 겹치는 부분이 많다.

```{r}
# confusion matrix
table(wine[,2],k.means.fit$cluster)

table(wine[,2],k.means.fit.5$cluster)
```

<br>

####2-2. Hierarchical clustering 으로 접근

```{r}
d <- dist(df, method = "euclidean") # Euclidean계산법으로 거리행렬 구하기 
H.fit <- hclust(d, method = "ward.D2") # Ward.D2는 within-cluster variance를 최소화
                                       # hclust의 첫번째 인자는 요소간의 거리행렬이어야 함 

plot(H.fit) # dendogram 도식화
groups <- cutree(H.fit, k = 3) # cutree거리(h)나 군집 수(k) 기준으로 그룹화한 결과 데이터 얻기
rect.hclust(H.fit, k = 3, border = "red") # 군집의 수(k)가 3이 되도록 군집화된 결과 나누기 

# Confusion matrix
table(wine[,2],groups)
```

<br>

## k-means 그룹간 거리 측정하기 
```{r}
data(ruspini)

rusp.norm <- scale(ruspini) # 항상 Feature scaling 해주고 데이터 작업
rusp.norm.euclid <- dist(rusp.norm, method = "euclidean") 

par(mfrow = c(2, 1))
cl.sin <- hclust(rusp.norm.euclid, method = "single") #bottom up
cl.ave <- hclust(rusp.norm.euclid, method = "average") # top down
```

```{r}
plot(cl.sin, hang = -1)
```

```{r}
plot(cl.ave, hang = -1)
```

```{r}
par(mfrow = c(1, 1))

par(mar = c(5,4,1,1))
mycluster = function(data, cluster, K, ...) {
  col = cutree(cluster, k = K)
  plot(data, col = col, pch = 19, ...)
}
mycluster(ruspini, cl.sin, K = 2)
mycluster(ruspini, cl.ave, K = 2)
mycluster(ruspini, cl.sin, K = 3)
mycluster(ruspini, cl.ave, K = 3)

par(mfrow = c(1, 1))
```

```{r}
## 3. iris 데이터로 iteration 효과 확인
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()

km.5 <- kmeans(iris[,3:4], centers = 3, nstart = 10,
                iter.max = 5, algorithm = "Lloyd")
# 실행후 Warning messages : did not converge in 5 iterations 
plot(iris[,3:4], pch = 19, col = km.5$cluster)

km.10 <- kmeans(iris[,3:4], centers = 3, nstart = 10,
             iter.max = 10, algorithm = "Lloyd")
# iter.max(반복횟수)값을 늘려주니 converge(수렴) 함
plot(iris[,3:4], pch = 19, col = km.10$cluster)

km.20 <- kmeans(iris[,3:4], centers = 3, nstart = 10,
             iter.max = 20, algorithm = "Lloyd")
# 결과는 동일
plot(iris[,3:4], pch = 19, col = km.20$cluster)

## 4. Visualization tool 활용
## 4.1 k-medoids 의 효과 보기

fviz_nbclust(df, kmeans, method = "gap_stat") # 적정 k 값 찾기

k.means.fit <- kmeans(df, 3, nstart = 25) # k-means
fviz_cluster(k.means.fit, data = df, frame.type = "convex")+ theme_minimal()

pam.res <- pam(df, 3) # k-medoids
fviz_cluster(pam.res)  # 적용후 k-means 보다 덜 겹침

## 4.2 다른 데이터에 적용
data("USArrests")
my_data <- scale(USArrests)
d <- dist(my_data, method = "euclidean")
res.hc <- hclust(d, method = "ward.D2" ) #ward.D2s - bottom up
grp <- cutree(res.hc, k = 4)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 4, border = 2:5) # Cluster dendrogram 이 4개

# Dendrogram (hcut 함수 이용)
res.hcut <- hcut(USArrests, k = 4, stand = TRUE)
fviz_dend(res.hcut, rect = TRUE, cex = 0.5,
          k_colors = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"))

# Optimal k 찾기
fviz_nbclust(my_data, kmeans, method = "gap_stat")
res.nbclust <- NbClust(my_data, distance = "euclidean",
                       min.nc = 2, max.nc = 10, 
                       method = "complete", index ="all") 
factoextra::fviz_nbclust(res.nbclust) + theme_minimal()

## 5. 무슨 방법을 적용하는게 가장 좋은지 테스트

test.result <- clValid(my_data, nClust = 2:6, 
                  clMethods = c("hierarchical","kmeans","pam"),  # k-medoid~
                  validation = "internal")
summary(test.result)

```


